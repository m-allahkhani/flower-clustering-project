{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"ssbpOiH7ehhY","executionInfo":{"status":"ok","timestamp":1706781548891,"user_tz":-210,"elapsed":5929,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from torch.functional import F\n","from tqdm import trange\n","import numpy as np\n","from torch.nn import LogSoftmax"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zQbLWSAatBo3","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1706781541410,"user_tz":-210,"elapsed":9815,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}},"outputId":"5a65edd8-2c95-4b12-e5e3-143700b6f504"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D09qqlb7uFEN","outputId":"a584325b-c812-4ec9-91bf-3852103d60f8","executionInfo":{"status":"ok","timestamp":1706781553137,"user_tz":-210,"elapsed":434,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3Eznbbp6s5pZ","executionInfo":{"status":"ok","timestamp":1706781569327,"user_tz":-210,"elapsed":1590,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["from sample_data.utils import get_oxford_splits,custom_plot_training_stats"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NSu4EbXb1C-v","executionInfo":{"status":"ok","timestamp":1706781573090,"user_tz":-210,"elapsed":413,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["use_gpu = True\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\") # New\n"]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEqdO6brbfGI","outputId":"ab1dcd8d-b445-441b-96ad-98c982b0a9ce","executionInfo":{"status":"ok","timestamp":1706781575160,"user_tz":-210,"elapsed":2,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":10,"metadata":{"id":"FCsofFUaIUDD","executionInfo":{"status":"ok","timestamp":1706781828812,"user_tz":-210,"elapsed":401,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["class CNN_phase1(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = torch.nn.Sequential(\n","            #layer 1\n","            torch.nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(),\n","\n","\n","\n","            #layer 2\n","            torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=64),\n","            torch.nn.ReLU(),\n","\n","            #layer 3\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","\n","            #layer 4\n","            torch.nn.Conv2d(in_channels = 64, out_channels = 96, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=96),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 96, out_channels = 96, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=96),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 96, out_channels = 96, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=96),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 96, out_channels = 96, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=96),\n","            torch.nn.ReLU(),\n","\n","            #layer 5\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","            #layer 6\n","            torch.nn.Conv2d(in_channels = 96, out_channels = 128, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=128),\n","            torch.nn.ReLU(),\n","            #layer 7\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","            #layer 8\n","            torch.nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=256),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=256),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=256),\n","            torch.nn.ReLU(),\n","\n","            torch.nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n","            torch.nn.BatchNorm2d(num_features=256),\n","            torch.nn.ReLU(),\n","            #layer 9\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","            #layer 10 fc\n","            torch.nn.Flatten(),\n","            torch.nn.Linear(256*4*4, 80),\n","\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n"]},{"cell_type":"code","source":["class CNN_phase2(nn.Module):\n","    def __init__(self, pretrained_cnn):\n","        super(CNN_phase2, self).__init__()\n","\n","        # Create a new CNN model with the same convolutional layers as the pretrained model\n","        self.model = nn.Sequential(\n","            *list(pretrained_cnn.model.children())[:-1]  # Keep only convolutional layers, remove last 3 FC layers\n","        )\n","\n","        # Define new fully connected layers\n","        self.fc1 = nn.Linear(256*4*4, 100)  # Change the number of neurons to 100\n","        self.relu1 = nn.ReLU()\n","        self.fc1.weight.data[:80, :] = pretrained_cnn.model[-1].weight.data.view(80, -1)\n","        self.fc1.bias.data[:80] = pretrained_cnn.model[-1].bias.data[:80]\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        s= self.relu1(x)\n","        return x"],"metadata":{"id":"5C8hd61xY8LX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN_phase2_2(nn.Module):\n","    def __init__(self, pretrained_cnn):\n","        super(CNN_phase2_2, self).__init__()\n","\n","        # Create a new CNN model with the same convolutional layers as the pretrained model\n","        self.model = nn.Sequential(\n","            *list(pretrained_cnn.model.children())[:-1]  # Keep only convolutional layers, remove last 3 FC layers\n","        )\n","\n","        # Define new fully connected layers\n","        self.fc1 = nn.Linear(256*4*4, 100)  # Change the number of neurons to 100\n","        self.relu1 = nn.ReLU()\n","        self.fc1.weight.data[:80, :] = pretrained_cnn.model[-1].weight.data.view(80, -1)\n","        self.fc1.bias.data[:80] = pretrained_cnn.model[-1].bias.data[:80]\n","\n","        # Initialize the weights of the new fully connected layer using the weights from the pretrained model\n","\n","        for param in self.model[:-2].parameters():\n","                  param.requires_grad = False\n","\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x"],"metadata":{"id":"vLK5krZkpjAr","executionInfo":{"status":"ok","timestamp":1706781873884,"user_tz":-210,"elapsed":374,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class CNN_phase2_3(nn.Module):\n","    def __init__(self, pretrained_cnn):\n","        super(CNN_phase2_3, self).__init__()\n","\n","        # Create a new CNN model with the same convolutional layers as the pretrained model\n","        self.model = nn.Sequential(\n","            *list(pretrained_cnn.model.children())[:-1]  # Keep only convolutional layers, remove last 3 FC layers\n","        )\n","\n","        # Define new fully connected layers\n","        self.fc1 = nn.Linear(256*4*4, 100)  # Change the number of neurons to 100\n","        self.relu1 = nn.ReLU()\n","        self.fc1.weight.data[:80, :] = pretrained_cnn.model[-1].weight.data.view(80, -1)\n","        self.fc1.bias.data[:80] = pretrained_cnn.model[-1].bias.data[:80]\n","\n","        def freeze_weights(self, percentage=0.8):\n","            total_weights = self.fc1.weight.numel()\n","            weights_to_freeze = int(percentage * total_weights)\n","\n","            # Determine which weights to freeze\n","            weights_frozen = 0\n","            for param in self.fc1.parameters():\n","                if weights_frozen < weights_to_freeze:\n","                   param.requires_grad = False\n","                   weights_frozen += param.numel()\n","                else:\n","                     break\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x\n"],"metadata":{"id":"WSqyefCJ0vo8","executionInfo":{"status":"ok","timestamp":1706781914090,"user_tz":-210,"elapsed":372,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"9xnG1Md1fKlW","executionInfo":{"status":"ok","timestamp":1706781921270,"user_tz":-210,"elapsed":343,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["def train_one_epoch(model: nn.Module, optim: torch.optim.Optimizer,\n","         dataloader: DataLoader, loss_fn):\n","    num_samples = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    running_corrects = 0\n","    running_loss = 0.0\n","\n","    model.train()\n","\n","    for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        outputs = model(inputs) # Forward Pass, [N, 10]\n","        loss = loss_fn(outputs, targets) # Compute Loss\n","\n","        loss.backward() # Compute Gradients\n","        optim.step() # Update parameters\n","        optim.zero_grad() # zero the parameter's gradients\n","\n","        _, preds = torch.max(outputs, dim=1) # Explain, [N]\n","        running_corrects += torch.sum(preds == targets)\n","        running_loss += loss.item()\n","\n","        if batch_indx == 0:\n","            print(outputs.device)\n","\n","    epoch_acc = (running_corrects / num_samples) * 100\n","    epoch_loss = (running_loss / num_batches)\n","\n","    return epoch_acc, epoch_loss"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wayOUgLGfQVr","executionInfo":{"status":"ok","timestamp":1706781935523,"user_tz":-210,"elapsed":434,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["def test_model(model: nn.Module,\n","         dataloader: DataLoader, loss_fn):\n","\n","    # utils\n","    num_samples = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    running_corrects = 0\n","    running_loss = 0.0\n","\n","    model.eval() # you must call `model.eval()` to set dropout and batch normalization layers to evaluation mode before running inference.\n","    with torch.no_grad(): # explain\n","        # more on torch.no_grad(): https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#disabling-gradient-tracking\n","\n","        for batch_indx, (inputs, targets) in enumerate(dataloader): # Get a batch of Data\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            outputs = model(inputs) # Forward Pass\n","            loss = loss_fn(outputs, targets) # Compute Loss\n","\n","            # loss.backward() # Compute Gradients\n","            # optim.step() # Update parameters\n","            # optim.zero_grad() # zero the parameter's gradients\n","\n","            _, preds = torch.max(outputs, 1) #\n","            running_corrects += torch.sum(preds == targets)\n","            running_loss += loss.item()\n","\n","\n","            if batch_indx == 0:\n","                print(outputs.device)\n","\n","    test_acc = (running_corrects / num_samples) * 100\n","    test_loss = (running_loss / num_batches)\n","\n","    return test_acc, test_loss\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"K2e6lakIfSrv","executionInfo":{"status":"ok","timestamp":1706781939223,"user_tz":-210,"elapsed":352,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"outputs":[],"source":["# from utils import custom_plot_training_stats\n","\n","def phase1():\n","    batch_size = 128\n","\n","    learning_rate = 0.001\n","\n","    A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(batch_size=128)\n","\n","    trainset = A_train_dl.dataset  # Access the dataset from the DataLoader\n","    testset = A_test_dl.dataset    # Access the dataset from the DataLoader\n","\n","    full_dataloaders = {\n","        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(testset, batch_size=batch_size)\n","    }\n","\n","\n","    model = CNN_phase1()\n","    model = model.to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    cross_entropy = nn.CrossEntropyLoss()\n","\n","    acc_history = {'train': [], 'test': []}\n","    loss_history = {'train': [], 'test': []}\n","\n","    for epoch in trange(15):\n","        train_acc, train_loss = train_one_epoch(model=model, optim=optimizer, dataloader=full_dataloaders['train'], loss_fn=cross_entropy)\n","        test_acc, test_loss = test_model(model=model, dataloader=full_dataloaders['test'], loss_fn=cross_entropy)\n","\n","        acc_history['train'].append(train_acc)\n","        acc_history['test'].append(test_acc)\n","        loss_history['train'].append(train_loss)\n","        loss_history['test'].append(test_loss)\n","    acc_history['train'] =  [round(float(tensor.cpu()), 4) for tensor in acc_history['train']]\n","    acc_history['test'] = [round(float(tensor.cpu()), 4) for tensor in acc_history['test']]\n","    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='cnn1', dir='phase1_plots')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8z0TZtpLwE4"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"id":"vQ44OIDXfv3a","outputId":"7102c073-096c-447b-9971-d8c7cb84c425","executionInfo":{"status":"error","timestamp":1706773561145,"user_tz":-210,"elapsed":23116,"user":{"displayName":"MAT. Toopkanlu","userId":"09657092489693953850"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/10 [00:22<?, ?it/s]\n"]},{"output_type":"error","ename":"TypeError","evalue":"cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not NoneType","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-6d41dd537410>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphase1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-8f6bbde7d9b2>\u001b[0m in \u001b[0;36mphase1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# while not end:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_dataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_dataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-46c07ea26f42>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optim, dataloader, loss_fn)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Forward Pass, [N, 10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not NoneType"]}],"source":["model = phase1()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95Pva-2biYfF"},"outputs":[],"source":["\n","# from utils import custom_plot_training_stats\n","\n","def phase2():\n","    batch_size = 128\n","    num_epochs = 10\n","    learning_rate = 0.005\n","    A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(batch_size=128)\n","    trainset = A_train_dl.dataset  # Access the dataset from the DataLoader\n","    testset = A_test_dl.dataset    # Access the dataset from the DataLoader\n","\n","    full_dataloaders1 = {\n","        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(testset, batch_size=batch_size)\n","    }\n","    full_dataloaders2 = {\n","        'train': DataLoader(B_train_dl.dataset , batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(B_test_dl.dataset , batch_size=batch_size)\n","    }\n","\n","    pretrained_model = CNN_phase1()\n","\n","    pretrained_model = pretrained_model.to(device)\n","    cross_entropy = nn.CrossEntropyLoss()\n","    optimizer2 = torch.optim.Adam(pretrained_model.parameters(), lr=learning_rate)\n","    for epoch in trange(2):\n","        train_acc, train_loss = train_one_epoch(model=pretrained_model, optim=optimizer2, dataloader=full_dataloaders1['train'], loss_fn=cross_entropy)\n","    new_model = CNN_phase2(pretrained_model)\n","    new_model = new_model.to(device)\n","    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n","\n","\n","    loss_history = {'train': [], 'test': []}\n","    acc_history = {'train': [], 'test': []}\n","\n","    for epoch in trange(1):\n","        train_acc, train_loss = train_one_epoch(model=new_model, optim=optimizer, dataloader=full_dataloaders2['train'], loss_fn=cross_entropy)\n","        test_acc, test_loss = test_model(model=new_model, dataloader=full_dataloaders2['test'], loss_fn=cross_entropy)\n","        acc_history['train'].append(train_acc)\n","        acc_history['test'].append(test_acc)\n","        loss_history['train'].append(train_loss)\n","        loss_history['test'].append(test_loss)\n","\n","    # Convert the tensors to plain Python numbers\n","    acc_history[\"train\"] =  [round(float(tensor.cpu()), 4) for tensor in acc_history[\"train\"]]\n","    acc_history[\"test\"] = [round(float(tensor.cpu()), 4) for tensor in acc_history[\"test\"]]\n","\n","    # print(\"-----------\")\n","    print(loss_history)\n","    print(acc_history)\n","    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='cnn1', dir='phase2_plots')"]},{"cell_type":"code","source":["new_model = CNN_phase2(pretrained_model)\n","new_model = new_model.to(device)\n","inputs = inputs.to(device)\n","inputs = inputs.to(device)"],"metadata":{"id":"iLzzDkt5qE_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phase2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"id":"UMsysv87aYux","outputId":"7d241802-e6fb-4016-cb66-99e918834c77","executionInfo":{"status":"ok","timestamp":1706777216127,"user_tz":-210,"elapsed":57029,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/2 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 1/2 [00:26<00:26, 26.78s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2/2 [00:53<00:00, 26.56s/it]\n","  0%|          | 0/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n","cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"]},{"output_type":"stream","name":"stdout","text":["{'train': [4.684528350830078], 'test': [5.271416282653808]}\n","{'train': [0.0], 'test': [1.9305]}\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1400x600 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["\n","# from utils import custom_plot_training_stats\n","\n","def phase2_2():\n","    batch_size = 128\n","    num_epochs = 10\n","    learning_rate = 0.005\n","    A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(batch_size=128)\n","    trainset = A_train_dl.dataset  # Access the dataset from the DataLoader\n","    testset = A_test_dl.dataset    # Access the dataset from the DataLoader\n","\n","    full_dataloaders1 = {\n","        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(testset, batch_size=batch_size)\n","    }\n","    full_dataloaders2 = {\n","        'train': DataLoader(B_train_dl.dataset , batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(test_all.dataset , batch_size=batch_size)\n","    }\n","\n","    pretrained_model = CNN_phase1()\n","\n","    pretrained_model = pretrained_model.to(device)\n","    cross_entropy = nn.CrossEntropyLoss()\n","    optimizer2 = torch.optim.Adam(pretrained_model.parameters(), lr=learning_rate)\n","    for epoch in trange(5):\n","        train_acc, train_loss = train_one_epoch(model=pretrained_model, optim=optimizer2, dataloader=full_dataloaders1['train'], loss_fn=cross_entropy)\n","    new_model = CNN_phase2_2(pretrained_model)\n","    new_model = new_model.to(device)\n","    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n","\n","\n","    loss_history = {'train': [], 'test': []}\n","    acc_history = {'train': [], 'test': []}\n","\n","    for epoch in trange(3):\n","        train_acc, train_loss = train_one_epoch(model=new_model, optim=optimizer, dataloader=full_dataloaders2['train'], loss_fn=cross_entropy)\n","        test_acc, test_loss = test_model(model=new_model, dataloader=full_dataloaders2['test'], loss_fn=cross_entropy)\n","        acc_history['train'].append(train_acc)\n","        acc_history['test'].append(test_acc)\n","        loss_history['train'].append(train_loss)\n","        loss_history['test'].append(test_loss)\n","\n","    # Convert the tensors to plain Python numbers\n","    acc_history[\"train\"] =  [round(float(tensor.cpu()), 4) for tensor in acc_history[\"train\"]]\n","    acc_history[\"test\"] = [round(float(tensor.cpu()), 4) for tensor in acc_history[\"test\"]]\n","\n","    # print(\"-----------\")\n","    print(loss_history)\n","    print(acc_history)\n","    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='cnn1', dir='phase2_plots')"],"metadata":{"id":"VBRHWPOSqHnL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phase2_2()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"mRx17yRMqOkf","outputId":"e2ad9fdd-f4ad-4944-954b-42cc75cb53ed","executionInfo":{"status":"ok","timestamp":1706777548877,"user_tz":-210,"elapsed":163045,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 1/5 [00:27<01:48, 27.04s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 2/5 [00:53<01:19, 26.61s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 3/5 [01:19<00:52, 26.40s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 4/5 [01:46<00:26, 26.49s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [02:12<00:00, 26.58s/it]\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n","cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [00:10<00:20, 10.35s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n","cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [00:20<00:10, 10.27s/it]"]},{"output_type":"stream","name":"stdout","text":["cuda:0\n","cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [00:30<00:00, 10.04s/it]\n"]},{"output_type":"stream","name":"stdout","text":["{'train': [4.626165390014648, 3.9566760063171387, 3.5772006511688232], 'test': [8.136904856737923, 9.878483603982364, 10.465986111584831]}\n","{'train': [1.0, 11.0, 12.0], 'test': [0.5837, 2.286, 1.2646]}\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1400x600 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","source":["\n","# from utils import custom_plot_training_stats\n","\n","def phase2_3():\n","    batch_size = 128\n","    num_epochs = 10\n","    learning_rate = 0.005\n","    A_train_dl, A_test_dl, B_train_dl, B_test_dl, test_all = get_oxford_splits(batch_size=128)\n","    trainset = A_train_dl.dataset  # Access the dataset from the DataLoader\n","    testset = A_test_dl.dataset    # Access the dataset from the DataLoader\n","\n","    full_dataloaders1 = {\n","        'train': DataLoader(trainset, batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(testset, batch_size=batch_size)\n","    }\n","    full_dataloaders2 = {\n","        'train': DataLoader(B_train_dl.dataset , batch_size=batch_size, shuffle=True),\n","        'test': DataLoader(test_all.dataset , batch_size=batch_size)\n","    }\n","\n","    pretrained_model = CNN_phase1()\n","\n","    pretrained_model = pretrained_model.to(device)\n","    cross_entropy = nn.CrossEntropyLoss()\n","    optimizer2 = torch.optim.Adam(pretrained_model.parameters(), lr=learning_rate)\n","    for epoch in trange(3):\n","        train_acc, train_loss = train_one_epoch(model=pretrained_model, optim=optimizer2, dataloader=full_dataloaders1['train'], loss_fn=cross_entropy)\n","    new_model = CNN_phase2_3(pretrained_model)\n","    new_model = new_model.to(device)\n","    optimizer = torch.optim.Adam(new_model.parameters(), lr=learning_rate)\n","\n","\n","    loss_history = {'train': [], 'test': []}\n","    acc_history = {'train': [], 'test': []}\n","\n","    for epoch in trange(2):\n","        train_acc, train_loss = train_one_epoch(model=new_model, optim=optimizer, dataloader=full_dataloaders2['train'], loss_fn=cross_entropy)\n","        test_acc, test_loss = test_model(model=new_model, dataloader=full_dataloaders2['test'], loss_fn=cross_entropy)\n","        acc_history['train'].append(train_acc)\n","        acc_history['test'].append(test_acc)\n","        loss_history['train'].append(train_loss)\n","        loss_history['test'].append(test_loss)\n","        for name, param in model.named_parameters():\n","            print(f\"Epoch [{epoch+1}] - Layer: {name} - Weights: {param.data}\")\n","\n","\n","\n","    # Convert the tensors to plain Python numbers\n","    acc_history[\"train\"] =  [round(float(tensor.cpu()), 4) for tensor in acc_history[\"train\"]]\n","    acc_history[\"test\"] = [round(float(tensor.cpu()), 4) for tensor in acc_history[\"test\"]]\n","\n","    # print(\"-----------\")\n","    print(loss_history)\n","    print(acc_history)\n","    custom_plot_training_stats(acc_history, loss_history, ['train', 'test'], title='cnn1', dir='phase2_plots')"],"metadata":{"id":"e-RLXSgJw25z","executionInfo":{"status":"ok","timestamp":1706781955625,"user_tz":-210,"elapsed":354,"user":{"displayName":"shaghayegh shafiee","userId":"07110522800366405567"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["phase2_3()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNfIRiYLw_W8","outputId":"2aef94c7-faf9-45a9-f7e6-4301b7185a76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to /content/data/flowers-102/102flowers.tgz\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 96174080/344862509 [00:04<00:08, 30117791.74it/s]"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}